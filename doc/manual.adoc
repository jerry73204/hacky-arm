= The Hacky Robot Arm Project
:author: Hsiang-Jui Lin, Yu-Yuan Yuan
:data-uri:
== Overview

The project aims to unleash the power of Dobot Magician arm with Intel RealSense D435 depth sensor.
With our specially designed algorithm, the robot arm is able to grab the object without human intervention,
and judge the depth image from RealSensoe sensor to adjust the pose.

Our project deliver the following items:

Core Controller::
The program scans the video and depth video in real time. It allows user to issue commands to grab objects either manually or automatically. +
It features *real-time object detection*, *object depth estimation* and *point cloud visualization*. +
It is highly configurable. Users can tweak the parameters for object detection and robot controller.

Object Detector Tweaking Tool::
Since the core controller relies on image processing to determine the position of object. +
The utility program lets users to tweak the parameters, and preview the detection performance interactively. +
The output parameters can be directly loaded by the core controller.

Camera to Robot Measurement Tool::
The coordinates of objects differ in Dobot arm's and camera's coordinate system. This tool instructs users to measure +
the corresponding position of a object in both arm and camera coordinates, then, computes the affine transformation.

Specially-designed Hardware::
We design a support to fix the RealSense sensor on end-effector. It can be 3D-printed. It can be directly installed on Dobot without modification on Dobot and RealSense sensor.

== Achievements

Our robot arm is able to achive the following tasks.

- Move the robot arm to the center of object within 2 cm error.
- Estimate the height of object up to 11 cm height using depth sensor, and adjust the robot arm accordingly.
- Process up to 20 images per second, including object detection and depth estimation on Jetson Nano. The actual performance depends on system loading and hardware.

== Usage

The core controller requires correct parameters to reliably perform object detection and robot arm movement.
It's necessary to go through a series parameter tweaking process before we invoke the controller.
The procedure goes as the following.

1. Run the object detector tweaking tool. Tweak the parameters until it can accurately estimate the object position.
2. Run the camera to robot measurement tool. It measures the affine transformation matrix that converts objects in image coordinates to robot coordinates.
3. Save all previous measured parameters from previous steps.
4. Load the parameters from previous step and start the core controller.

== Techninal Details

=== The Program Architecture

Our program copes with several I/O and computing devices simultaneously. For example, it scans video and depth images from the RealSense sensor,
prints status to terminal, handles user key input, sends commands to Dobot, and executes computation-heavy object detection and visualizations.
Each device goes at its pace, and thus, the synchronization between them was a challenge to us. We developed a asynchronous multi-worker architecture.
The <<the-program-architecture, figure>> below illustrates the overall architecture of our program.

[#the-program-architecture]
.The Program Architecture
[graphviz, "image/program-architecture", png]
....
digraph {
    node [shape=box, fontsize=16, width=0.9];
    edge [fontsize=16];
    rankdir=LR;

    subgraph cluster_1 {
        label = "legend";
        "asynchronous worker";
        "queue" [shape="trapezium" color="orange"];
    }

    subgraph cluster_2 {
        style = "invis"
        "queue1" [shape="trapezium" color="orange", label = "lossy"];
        "queue2" [shape="trapezium" color="orange", label = "lossy"];
        "queue3" [shape="trapezium" color="orange", label = "lossless"];
        "viz_queue" [shape="trapezium" color="orange", label = "lossy"];
        "ctrl_queue" [shape="trapezium" color="orange", label = "timeout"];

        "RealSense sensor" [shape="octagon", color="red"];
        "Dobot" [shape="octagon", color="red"];
        "shared state" [shape="circle"];

        "RealSense sensor" -> "RealSense consumer";
        "Dobot worker" -> "Dobot";

        "RealSense consumer" -> "queue1";
        "queue1" -> "Object detector";

        "Object detector" -> "queue2"
        "queue2" -> "Controller";

        "Controller" -> "queue3"
        "Auto controller" -> "queue3"
        "queue3" -> "Dobot worker";

        "shared state" -> "Controller" [color = "gray"];
        "shared state" -> "Auto controller" [color = "gray"];
        "shared state" -> "Visualizer" [color = "gray"];
        
        "viz_queue" -> "Visualizer";
        "RealSense consumer" -> "viz_queue";
        "Object detector" -> "viz_queue";
        "Controller" -> "viz_queue";

        "Visualizer" -> "ctrl_queue";
        "ctrl_queue" -> "Controller";
        }
}
....

Our architecture features several spotlights.

- We work with link:https://www.rust-lang.org/[Rust language] to build our program.
Rust provides type safety to help us eliminate data racing among threads, and
help us and avoid unsafe memory operations such as accessing destructed object and invalid location.
We thus can build the framework with solid safety and with confidence.

- We adopted the link:https://github.com/tokio-rs/tokio[tokio] asynchronous runtime.
It provides various kinds of channels to help us connect multiple workers together.
We use lossy broadcast channel among all workers from RealSense consuemr to visualizer.
It ensures the input images are shown on screen in real time.

- We put lots of consideration on interaction between the visualizer and the controller.
The visualizer receives user commands and send to the controller. Commands should be processed soon for responsiveness.
However, the Dobot worker would be busy grabbing an object. The controller would wait for the Dobot for a long time.
while the user command would wait for a long time to be processed. It would confuse the user and should be prevented.
We borrow a lossy channel from tokio, where each command is marked with a TTL. It ensures the controller always process
the most recent command for better responsiveness.

- Our program has a _automatic mode_, in which our program seek for objects, move them to another place, and repeat to move them back.
The user can enable or disable auto mode, and call Dobot to calibrate itself during auto mode. In the mean time, the visualizer
presents the state of the controller. To achieve this, we build a shared state watched by the controller and the visualizer.
Whenever the state is changed, such as if the Dobot is busy or not, the visualizer is notified and updates the visualization accordingly.

=== Object Detection Process

The object detection relies on OpenCV primitives. The main stages are

1. *HSV thresholding*: Namely, filter pixels by color brightness and satuation.
2. *Morphology transformations*: It effectively reduce the noise points.
3. *Contour detection*: It find components of connected pixels as objects.
4. *Reject bad contours*: It removes objects out of range of interest and those with small area.

The resulting recall is affected by the ambient light and the texture of object. It's suggested to run the object detection tweaking tool to find the proper parameters.

=== Object Depth Estimation



The depth estimation is mainly done in these steps.

1. First, we call the object detector to locate objects on images.
2. Then, for each object, find the corresponding depth pixels in depth image.
3. Compute the depth for that object from collected depth pixels.

It is somewhat a complex job because the depth and color image are captured at distinct aspect, and their actual
time would slightly differ. Thanks to link:https://github.com/IntelRealSense/librealsense[librealsense], it has all essential tools
to help us align the color and depth images. To integrate the library with our architecture, we developed
the Rust binding for librealsense and make it public on link:https://github.com/jerry73204/realsense-rust[GitHub].
