= The Hacky Robot Arm Project
:author: 林祥瑞, 袁佑緣
:data-uri:

== 專題介紹

本專題主要使用Dobot Magician機械手臂結合Intel RealSense D435深度感測器。得益於精巧的演算法，我們的機械手臂
可以不需要任何人為操作即可自行夾取物件，並且透過深度感測器來適度地調整位置。


本專題由以下幾項要素所構成：

核心控制器::
程式會即時接收彩色與深度影像。並支援讓使用者調整手動或自動夾取的功能。我們目前實做的特點有
*即時物件辨識*， *物件深度估計* 以及 *點雲資料視覺化*。 +
此外，本程式也支援參數調整，使用者可以根據我們的輔助程式來適當微調物件偵測以及機器手臂的控制。

物件偵測參數調整工具::
由於我們的核心控制器主要是基於影像處理來找尋物件的位置，實務上可以藉由工具程式來讓使用者對於不同的物件
做細部的微調，並且支援人機界面來即時查看物件辨識的結果，最後調整的參數亦可以直接匯入主程式當中。

攝影鏡頭與機器人間的轉換測量工具::
由於同一個物件在機械手臂與鏡頭的座標系上會有所不同，我們也開發了一套工具程式來輔助使用者對物件的位置進行測量，
並設計一套模型來匹配最合適的仿射轉換。

攝影機與手臂的一體結合::
有別於以往需要多一個支架來架設第三人稱視角攝影機的設計，
我們精巧地用雷切壓克力製作出一個支架將深度攝影機直接固定在手臂末端。
同時為了能夠達到最佳的視野，我們也支援鏡頭在支架上面位置的細部微調。

== 目前成果

我們所設計的機械手臂目前可以完成以下的工作：

- 移動夾爪到物件中心點，誤差在2cm以內
- 使用深度攝影機來測量物件的高度，最高可達11cm，並根據測量的深度來適度調整機械手臂的夾取。
- 目前在Jetson Nano上面已經可以做到每秒即時處理20張影像，包含物件偵測與深度估計。
注意不同的控制板可能會表現有所不同，實際上的表現仍取決於系統負載與硬體效能。

== 如何使用

由於核心的控制程式需要針對不同物件有適當的參數才可以進行精準的物件偵測與手臂夾取，
我們設計了一系列的工具程式來輔助使用者來做參數微調。其概要如下：

1. 執行物件偵測調參，調整參數直到可以獲得正確的物件辨識。
2. 測量影像座標到機器人座標的轉換，我們將訓練一個模型來造出最合適的座標轉換。
3. 儲存所有的測量參數。
4. 將微調後的參數讀進主程式中。

詳細的操作與說明可以參考link:https://jerry73204.github.io/hacky-arm/calibration-ch.html[這裡]。


== 技術細節

=== 程式架構

我們的程式會同時處理各種的I/O與計算。舉例來說，從使用RealSense攝影機獲取彩色影像與深度影像，
再到印出資訊到終端，並處理使用者的鍵盤數入，送出控制訊息給機械手臂，以及執行一系列需要繁重計算
的物件偵測與視覺化。由於每一個裝置都有各自的流程，在兼顧效能下去處理各元件間訊息的同步是非常
具挑戰性的嘗試，而這也是我們選擇Rust語言的原因。以下是我們所設計的異同步多工的架構圖。

[#the-program-architecture]
.The Program Architecture
[graphviz, "image/program-architecture", png]
....
digraph {
    node [shape=box, fontsize=16, width=0.9];
    edge [fontsize=16];
    rankdir=LR;

    subgraph cluster_1 {
        label = "legend";
        "asynchronous worker";
        "queue" [shape="trapezium" color="orange"];
    }

    subgraph cluster_2 {
        style = "invis"
        "queue1" [shape="trapezium" color="orange", label = "lossy"];
        "queue2" [shape="trapezium" color="orange", label = "lossy"];
        "queue3" [shape="trapezium" color="orange", label = "lossless"];
        "viz_queue" [shape="trapezium" color="orange", label = "lossy"];
        "ctrl_queue" [shape="trapezium" color="orange", label = "timeout"];

        "RealSense sensor" [shape="octagon", color="red"];
        "Dobot" [shape="octagon", color="red"];
        "shared state" [shape="circle"];

        "RealSense sensor" -> "RealSense consumer";
        "Dobot worker" -> "Dobot";

        "RealSense consumer" -> "queue1";
        "queue1" -> "Object detector";

        "Object detector" -> "queue2"
        "queue2" -> "Controller";

        "Controller" -> "queue3"
        "Auto controller" -> "queue3"
        "queue3" -> "Dobot worker";

        "shared state" -> "Controller" [color = "gray"];
        "shared state" -> "Auto controller" [color = "gray"];
        "shared state" -> "Visualizer" [color = "gray"];

        "viz_queue" -> "Visualizer";
        "RealSense consumer" -> "viz_queue";
        "Object detector" -> "viz_queue";
        "Controller" -> "viz_queue";

        "Visualizer" -> "ctrl_queue";
        "ctrl_queue" -> "Controller";
        }
}
....

以上的架構有幾個亮點列舉如下：

- 我們主要使用link:https://www.rust-lang.org/[Rust]語言來開發我們的程式。Rust提供了型別安全性使得我們得以
更好的開發多線程程式而不用擔心資料爭用的問題。此外，也幫助開發者可以避免危險的記憶體操作如使用已註銷的物件或
不合法的記憶體位置。

- 我們使用tokio套件來作為異同步程式的框架，它提供了各種不同的通道來讓我們連接多個工作者(Worker)。
而我們使用有損(lossy)的廣播通道在各個RealSense與視覺化工作間的工作者間傳遞訊息。這使得剛輸入的影像可以被即時地呈現在畫面上。

- 我們花了很多的心力在設計畫圖工作(Visualizer)與控制器(Controller)之間的互動。
Visualizer首先會接收使用者的指令，並即地處理後傳送給Controller。然而，對於Dobot的工作而言，夾取物件是一件需要時間完成的工作，
所以Controller必須等待一段時間後才能完成夾取的動作。為了同時兼顧使用者的控制，程式會暫時關掉使用者的控制。我們使用tokio
提供的通道來設計說每一個命令都有標記TTL(time-to-live)，這使得控制器隨時都會處理最新的命令，因而做到即時的反應。

- 我們的程式也實做了_自動模式_，機械人會自動搜尋物體，並將各個辨識到的物件夾取到令一個指定的地點，直到所有的東西都搬過去另外一邊，
便會改變方向，將另外一邊的物件再次搬回來。使用者可以啟用或關閉這個模式，或者是叫機器人執行校正手臂的動作。在所有動作的同時，
Visualizer會隨時在螢幕上顯示控制的資訊。這類的功能主要是透過在Visualizer跟Controller構造一個共享狀態的訊息，並且只要狀態被改變的話，
例如：當機械手臂正忙於執行夾取時，Visualizer就會根據其狀況來更新資訊。


=== 物件偵測


我們實做的物件偵測主要是基於OpenCV函式庫，其主要的演算法構造如下：

1. *HSV thresholding*: 先將RGB彩色圖像轉換成HSV的表示，並加一個遮罩來做閾值化。
其遮罩主要是透過限制色域、亮度和飽和度。
2. *Morphology transformations*: 使用一些形態轉換來有效去除影像中的雜訊或破碎的部份。
3. *Contour detection*: 使用輪廓偵測來判定那些相連通得物件為欲辨識的物體。
4. *Reject bad contours*: 為了達到更精準的辨識結果，我們還額外使用了ROI(region of interest)來限制辨識範圍及
最大最小的物件週長來限制可能的辨識物件。

注意到物件辨識會受到環境光以及物體表面紋理的影像，我們建議最好參照上述關於使用輔助程式來微調參數的說明，以達到最佳的辨識結果。


=== 測量物件深度

關於如何準確測量到一個物體的深度可以細分成如下：

1. 呼叫物件辨識來找尋一張彩色影像中可能的物件位置。
2. 接下來，對於每一個物件去找尋相匹配的深度影像。
3. 根據找到那幾個像素點上面的深度來去計算，最後獲得準確的物件深度。

實務上，準確的測量物件深度是一件不簡單的事情，因為深度影像與彩色影像實際上是由不同的鏡頭角度所獲得，
且在處理上各幀影像的時間點有可能是不一樣的，得做對應的配對。對此，我們基於link:https://github.com/IntelRealSense/librealsense[librealsense]
函式庫（其上已經有具備必要的套件來對齊深度與彩色影像）來開發一個Rust版的RealSense函式庫。
本函式庫已開源link:https://github.com/jerry73204/realsense-rust[於此]。
